import os
import paramiko
import glob
import re
from collections import Counter
import fnmatch
import pandas as pd
from collections import defaultdict

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
s3_key_id = os.environ.get('AWS_ACCESS_KEY')
s3_access_key = os.environ.get('AWS_SECRET_KEY')

S3 = S3RemoteProvider(
    endpoint_url='https://s3.msi.umn.edu', 
    access_key_id=s3_key_id, 
    secret_access_key=s3_access_key
)

from snakemake.remote.SFTP import RemoteProvider as SFTPRemoteProvider
key = paramiko.agent.Agent().get_keys()[0]

SFTP = SFTPRemoteProvider(username='cull0084',private_key=key)

configfile: 'config.yaml' 

SUBSET = ['1M', '12F', '17M', '20M', '35F', '35M', '36F', '37M', '39F', '39M', 
          '40M', '49M', '51F', '52M', '61M', '65F', '65M', '66F', '66M', '67F', 
          '67M', '69F', '82M', '86M', '87M', '90M']
#SUBSET = ['12F', '17M', '1M', '20M', '35F']

#hiseq = False
#
#if hiseq:
#    PLATFORM = 'hiseq'
#    PROJECT_PATH = 'login.msi.umn.edu/panfs/roc/data_release/3/umgc/pre2018/2015-q4/mccuem/hiseq/151006_D00635_0082_BC7HAHANXX/Project_McCue_Project_022'
#    ALL_SAMPLES, = SFTP.glob_wildcards(f'{os.path.join(PROJECT_PATH, "{sample}_R2_001.fastq")}')
#    ZIPPED = ''
#    SAMPLES = [i for i in ALL_SAMPLES for j in SUBSET if j == i.split('_')[0]]
##    print(PROJECT_PATH)
##    print(SAMPLES)
#else:
#    PLATFORM = 'novaseq'
#    PROJECT_PATH = 'login.msi.umn.edu/panfs/roc/data_release/3/umgc/mccuem/novaseq/181112_A00223_0050_AHCWF7DSXX/McCue_Project_032'
#    ALL_SAMPLES, = SFTP.glob_wildcards(f'{os.path.join(PROJECT_PATH, "{sample}_R2_001.fastq.gz")}')
#    ZIPPED = '.gz'
#    SAMPLES = []
#    for i in SUBSET:
#        i = i[:-1] + '_' + i[-1]
#        for j in ALL_SAMPLES:
#            if i == '_'.join(j.split('_', 2)[:2]):
#                SAMPLES.append(j)
#    print(SAMPLES)

#HorseGeneAnnotation/private/sequence/RNASEQ/bam/Equus_caballus.EquCab3.0.95/paired_end/novaseq/STAR


PRIV_BUCKET = config['BUCKET']['PRIVATE']
PUB_BUCKET = config['BUCKET']['PUBLIC']
PLATFORM = 'hiseq'
#ASSEMBLY = config['ASSEMBLY']['NCBI']
ASSEMBLY = config['ASSEMBLY']['ENSEMBL']

LOCAL_PATH = f"{os.path.join(PRIV_BUCKET,ASSEMBLY,'paired_end',PLATFORM,'merged')}"
SAMPLES, = glob_wildcards(f"{os.path.join(LOCAL_PATH,'{samples}.merged.bam')}")

rule all:
    input:
        #expand(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/{{sample}}.gff',sample=SAMPLES)
        #f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/all_GFFs_list.txt'
        #f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged.gff'
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/transcript_fpkm.tsv',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/gene_fpkm.tsv',keep_local=True)
#        f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/SalmonQuant/novaseq_TPM.tsv'

# ----------------------------------------------------------
#       Sort bams
# ----------------------------------------------------------

rule pe_sort_bam:
    input:
        bam = f'{PRIV_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/merged/{{sample}}.merged.bam'
    output:
        sortedbam = S3.remote(f'{PRIV_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/merged/sorted/{{sample}}.merged.sorted.bam',keep_local=True)
    shell:
        '''
            samtools view -u {input.bam} | samtools sort - -o {output.sortedbam}
        '''

# ----------------------------------------------------------
#       StringTie
# ----------------------------------------------------------

rule gunzip_gff:
    input:
        ref_gff_gz = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/{ASSEMBLY}_genomic.nice.gff.gz')
    output:
        ref_gff = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/{ASSEMBLY}_genomic.nice.gff',keep_local=True)
    shell:
        '''
            gunzip {input.ref_gff_gz}
        '''

rule pe_run_stringtie:
    input:
        bam = f'{PRIV_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/merged/sorted/{{sample}}.merged.sorted.bam',
        ref_gff = f'{PUB_BUCKET}/{ASSEMBLY}/{ASSEMBLY}_genomic.nice.gff'
    output:
        gff = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/{{sample}}.gff',keep_local=True),
    log:
	    S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/{{sample}}.log')
    shell:
        '''
            stringtie \
                {input.bam} \
                -G {input.ref_gff} \
                -o {output.gff}
        '''


rule create_merged_GFF_file:
    input:
        gffs = expand(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/{{sample}}.gff',sample=SAMPLES)
    output:
        gff_list = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/all_GFFs_list.txt',keep_local=True)
    log:
	    S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/all_GFFs_list.log')
    run:
        with open(output.gff_list,'w') as OUT:
            print('\n'.join(input.gffs),file=OUT)


rule pe_stringtie_merge:
    input:
        gffs = expand(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/{{sample}}.gff',sample=SAMPLES),
	    ref_gff = f'{PUB_BUCKET}/{ASSEMBLY}/{ASSEMBLY}_genomic.nice.gff',
        gff_list = f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/all_GFFs_list.txt'
    output:
        merged_gff = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged.gff',keep_local=True)
    log:
	    S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged.log')
    shell:
        '''
            stringtie \
                --merge \
                -p 10 \
                -o {output.merged_gff} \
                -G {input.ref_gff} \
                {input.gff_list}
        '''


rule pe_stringtie_recalculate_on_merged:
    input:
        bam = f'{PRIV_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/merged/sorted/{{sample}}.merged.sorted.bam',
        merged_gff = f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged.gff'
    output:
        countsdir = directory(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/{{sample}}/counts'),
        gff = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/{{sample}}/{{sample}}.gff',keep_local=True)
    shell:
        '''
            stringtie \
                -e \
                -b {output.countsdir}\
                -G {input.merged_gff} \
                -o {output.gff} \
                {input.bam} &&
            mc cp -r {output.countsdir} s3/{output.countsdir}
        '''

# ----------------------------------------------------------
#       Make FPKM tables
# ----------------------------------------------------------
#
#rule download_counts:
#    input:
#        'HorseGeneAnnotation/public/refgen/{GCF}/paired_end/GFF/Merged/{sample}/counts/download.done'
#
#
#rule stringtie_counts:
#    input:
#        'HorseGeneAnnotation/public/refgen/{GCF}/paired_end/GFF/Merged/{sample}/counts/t_data.ctab'
#    output:
#        touch('HorseGeneAnnotation/public/refgen/{GCF}/paired_end/GFF/Merged/{sample}/counts/download.done')


rule pe_make_FPKM_tables:
    input:
        counts = expand(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/{{sample}}/counts',sample=SAMPLES)
    output:
        transcript_fpkm = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/transcript_fpkm.tsv'),
        gene_fpkm = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/GFF/Merged/gene_fpkm.tsv')
    run:
        import pandas as pd
        import numpy  as np
        dfs = []
        ctabs = [os.path.join(c,'t_data.ctab') for c in input.counts]
        for sample,f in zip(SAMPLES,ctabs):
            df = pd.read_table(f)
            df['sample'] = sample
            dfs.append(df)
        df = pd.concat(dfs)
        by_transcript = pd.pivot_table(df,index='t_name',columns='sample',values='FPKM')
        by_transcript.to_csv(output.transcript_fpkm,sep='\t')
        by_gene = pd.pivot_table(df,index='gene_name',columns='sample',values='FPKM',aggfunc=np.mean)
        by_gene.to_csv(output.gene_fpkm,sep='\t')


# ----------------------------------------------------------
#       SALMON indices
# ----------------------------------------------------------

rule download_SALMON:
    input:
        f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/download.done'
#        expand('HorseGeneAnnotation/public/refgen/{GCA}/SALMON_INDEX/download.done',GCA=config['ENSEMBL_GCA'])

rule SALMON_index:
    input:
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/duplicate_clusters.tsv',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/complete_ref_lens.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/seq.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/rank.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/reflengths.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/ctg_offsets.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/eqtable.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/ctable.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/refAccumLengths.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/refseq.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/info.json',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/pos.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/mphf.bin',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/versionInfo.json',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/ref_indexing.log',keep_local=True),
        S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/pre_indexing.log',keep_local=True)
    output:
        touch(f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/download.done')


# ----------------------------------------------------------
#       Make TPM matrix
# ----------------------------------------------------------

# need index

rule SALMON_mapping:
    input:
        R1 = f'HorseGeneAnnotation/private/sequence/trimmed/{PLATFORM}/{{sample}}_trim1.fastq.gz',
        R2 = f'HorseGeneAnnotation/private/sequence/trimmed/{PLATFORM}/{{sample}}_trim2.fastq.gz',
        salmon_dl = f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX/download.done'
    output:
#        directory('HorseGeneAnnotation/public/refgen/{GCF}/paired_end/SalmonQuant/{sample}'),
        f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/SalmonQuant/{{sample}}/quant.sf'
    params:
        salmon_index = f'{PUB_BUCKET}/{ASSEMBLY}/SALMON_INDEX',
        out_dir = f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/SalmonQuant/{{sample}}'
    run:
#        out_dir = os.path.dirname(output)
        shell('''
            salmon quant \
            -i {params.salmon_index} \
            -l A \
            -1 {input.R1} \
            -2 {input.R2} \
            --validateMappings \
            -o {params.out_dir}
        ''')


rule make_TPM_matrix:
    input:
        quant_files = expand(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/SalmonQuant/{{sample}}/quant.sf',sample=SAMPLES)
    output:
        tpm_matrix = S3.remote(f'{PUB_BUCKET}/{ASSEMBLY}/paired_end/{PLATFORM}/SalmonQuant/novaseq_TPM.tsv',keep_local=True)
#        fat_matrix = 'HorseGeneAnnotation/public/refgen/{GCF}/paired_end/SalmonQuant/Fat_TPM.tsv',
#        muscle_matrix = 'HorseGeneAnnotation/public/refgen/{GCF}/paired_end/SalmonQuant/Muscle_TPM.tsv'
    run:
        tpm = None
        for n in input.quant_files:                   
            name = n.split('/')[-2]
            print(name)
            #x = pd.read_table(f"data/SalmonQuant/{n}/quant.sf")                                         
            x = pd.read_table(n)
            x = x.loc[:,['Name','TPM']]            
            x.rename(columns={'Name':'gene','TPM':name},inplace=True)                                      
            x.set_index('gene',inplace=True)       
            if tpm is None:                        
                tpm = x                            
            else:                                  
                tpm = tpm.join(x,lsuffix='gene') 
        # Create the whole matrix
        tpm.to_csv(output.tpm_matrix,sep='\t')
        # Create the Fat & Muscle Matrix
#        tpm.loc[:,[x.endswith('F') for x in tpm.columns]].to_csv(output.fat_matrix,sep='\t')
#        tpm.loc[:,[x.endswith('M') for x in tpm.columns]].to_csv(output.muscle_matrix,sep='\t')
